{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/trnmah/.local/lib/python3.10/site-packages (from bitsandbytes) (1.24.3)\n",
      "Requirement already satisfied: torch in /home/trnmah/.local/lib/python3.10/site-packages (from bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: filelock in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\n",
      "Requirement already satisfied: sympy in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/trnmah/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/trnmah/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/trnmah/.local/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/trnmah/.local/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.2\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnf4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/quantization_config.py:399\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    397\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/quantization_config.py:457\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m ):\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/practic1/lib/python3.10/importlib/metadata/__init__.py:984\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m    978\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/mambaforge/envs/practic1/lib/python3.10/importlib/metadata/__init__.py:957\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    952\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \n\u001b[1;32m    954\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/practic1/lib/python3.10/importlib/metadata/__init__.py:548\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "\tload_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"VietAI/vit5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-large\", quantization_config= quantization_config)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-large\", quantization_config= quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config lora for model\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=False,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practic1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
